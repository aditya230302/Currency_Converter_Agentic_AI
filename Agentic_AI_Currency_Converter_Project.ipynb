{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing the Required Libraries**"
      ],
      "metadata": {
        "id": "2Um8R9DULw_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`langchain`**\n",
        "- LangChain is a framework used to build AI applications using large language models (LLMs).\n",
        "- It helps connect models with tools, memory, prompts, and agents."
      ],
      "metadata": {
        "id": "oluF-vp7Lz3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`langchain-google-genai`**\n",
        "- This library helps you use Google's Generative AI models (like Gemini) directly inside LangChain."
      ],
      "metadata": {
        "id": "mrCmjuMIOLhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`google-generativeai`**\n",
        "- Official Google library to access Gemini models using API keys without LangChain."
      ],
      "metadata": {
        "id": "xAfsxWdLOWMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`tiktoken`**\n",
        "- Used for counting tokens (words/characters) so you don’t exceed LLM token limits (important for pricing and requests)."
      ],
      "metadata": {
        "id": "IuRpMk2SOdLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`faiss-cpu`**\n",
        "- FAISS is a vector database used to store text embeddings and search relevant chunks quickly. Used in chatbots and RAG."
      ],
      "metadata": {
        "id": "Fu8NZkbsO6Q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`huggingface_hub`**\n",
        "- Allows downloading pre-trained models (like embedding models) from Hugging Face."
      ],
      "metadata": {
        "id": "dVs4ldcAO_Vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`langchain-community`**\n",
        "- Contains community-supported integrations like FAISS, agents, tools, vectorstores, embeddings, etc."
      ],
      "metadata": {
        "id": "DaNQAWlBPELx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`langgraph`**\n",
        "- Optional – Used for creating complex AI workflows or multi-step pipelines."
      ],
      "metadata": {
        "id": "-lbyYcI4PJq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-google-genai google-generativeai tiktoken faiss-cpu huggingface_hub langchain-community langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1Qxy0J8tPM9D",
        "outputId": "ea3125d7-3a68-4586-b239-6283dffdd53d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (2.1.12)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (0.6.10)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.78)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Collecting google-ai-generativelanguage<1,>=0.7 (from langchain-google-genai)\n",
            "  Using cached google_ai_generativelanguage-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filetype<2,>=1.2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Downloading google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Using cached langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain_google_genai-2.1.11-py3-none-any.whl.metadata (6.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.10-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.184.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.2)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.1.2)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.6.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.11.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-google-genai\n",
            "  Attempting uninstall: langchain-google-genai\n",
            "    Found existing installation: langchain-google-genai 2.1.12\n",
            "    Uninstalling langchain-google-genai-2.1.12:\n",
            "      Successfully uninstalled langchain-google-genai-2.1.12\n",
            "Successfully installed langchain-google-genai-2.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Insert API Key**"
      ],
      "metadata": {
        "id": "28TUJq5WQah3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# Used to store and access private values like API keys securely in Google Colab.\n",
        "key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "import google.generativeai as genai\n",
        "# This lets you directly use Gemini models with your API key (without LangChain).\n",
        "genai.configure(api_key= key)"
      ],
      "metadata": {
        "id": "BaQkL0TTQNPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Check available Models in google gen ai library**"
      ],
      "metadata": {
        "id": "Yj9DDuTDRWvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "available_models = genai.list_models()\n",
        "list(available_models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "CLBFeRAoQ0Q1",
        "outputId": "b5719382-1584-43b0-aba7-653bb1652a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-2.5-pro-preview-03-25',\n",
              "       base_model_id='',\n",
              "       version='2.5-preview-03-25',\n",
              "       display_name='Gemini 2.5 Pro Preview 03-25',\n",
              "       description='Gemini 2.5 Pro Preview 03-25',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-preview-05-20',\n",
              "       base_model_id='',\n",
              "       version='2.5-preview-05-20',\n",
              "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
              "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 2.5 Flash',\n",
              "       description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
              "                    'supports up to 1 million tokens, released in June of 2025.'),\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-lite-preview-06-17',\n",
              "       base_model_id='',\n",
              "       version='2.5-preview-06-17',\n",
              "       display_name='Gemini 2.5 Flash-Lite Preview 06-17',\n",
              "       description='Preview release (June 11th, 2025) of Gemini 2.5 Flash-Lite',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-pro-preview-05-06',\n",
              "       base_model_id='',\n",
              "       version='2.5-preview-05-06',\n",
              "       display_name='Gemini 2.5 Pro Preview 05-06',\n",
              "       description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-pro-preview-06-05',\n",
              "       base_model_id='',\n",
              "       version='2.5-preview-06-05',\n",
              "       display_name='Gemini 2.5 Pro Preview',\n",
              "       description='Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-pro',\n",
              "       base_model_id='',\n",
              "       version='2.5',\n",
              "       display_name='Gemini 2.5 Pro',\n",
              "       description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-exp',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash Experimental',\n",
              "       description='Gemini 2.0 Flash Experimental',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash',\n",
              "       description='Gemini 2.0 Flash',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-001',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash 001',\n",
              "       description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
              "                    'for scaling across diverse tasks, released in January of 2025.'),\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
              "       description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-lite-001',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash-Lite 001',\n",
              "       description='Stable version of Gemini 2.0 Flash-Lite',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-lite',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash-Lite',\n",
              "       description='Gemini 2.0 Flash-Lite',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-preview-image-generation',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash Preview Image Generation',\n",
              "       description='Gemini 2.0 Flash Preview Image Generation',\n",
              "       input_token_limit=32768,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
              "       base_model_id='',\n",
              "       version='preview-02-05',\n",
              "       display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
              "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-lite-preview',\n",
              "       base_model_id='',\n",
              "       version='preview-02-05',\n",
              "       display_name='Gemini 2.0 Flash-Lite Preview',\n",
              "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-pro-exp',\n",
              "       base_model_id='',\n",
              "       version='2.5-exp-03-25',\n",
              "       display_name='Gemini 2.0 Pro Experimental',\n",
              "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-pro-exp-02-05',\n",
              "       base_model_id='',\n",
              "       version='2.5-exp-03-25',\n",
              "       display_name='Gemini 2.0 Pro Experimental 02-05',\n",
              "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-exp-1206',\n",
              "       base_model_id='',\n",
              "       version='2.5-exp-03-25',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
              "       base_model_id='',\n",
              "       version='2.5-preview-05-20',\n",
              "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
              "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-thinking-exp',\n",
              "       base_model_id='',\n",
              "       version='2.5-preview-05-20',\n",
              "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
              "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
              "       base_model_id='',\n",
              "       version='2.5-preview-05-20',\n",
              "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
              "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-preview-tts',\n",
              "       base_model_id='',\n",
              "       version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
              "       display_name='Gemini 2.5 Flash Preview TTS',\n",
              "       description='Gemini 2.5 Flash Preview TTS',\n",
              "       input_token_limit=8192,\n",
              "       output_token_limit=16384,\n",
              "       supported_generation_methods=['countTokens', 'generateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-pro-preview-tts',\n",
              "       base_model_id='',\n",
              "       version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
              "       display_name='Gemini 2.5 Pro Preview TTS',\n",
              "       description='Gemini 2.5 Pro Preview TTS',\n",
              "       input_token_limit=8192,\n",
              "       output_token_limit=16384,\n",
              "       supported_generation_methods=['countTokens', 'generateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/learnlm-2.0-flash-experimental',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='LearnLM 2.0 Flash Experimental',\n",
              "       description='LearnLM 2.0 Flash Experimental',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=32768,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemma-3-1b-it',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemma 3 1B',\n",
              "       description='',\n",
              "       input_token_limit=32768,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemma-3-4b-it',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemma 3 4B',\n",
              "       description='',\n",
              "       input_token_limit=32768,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemma-3-12b-it',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemma 3 12B',\n",
              "       description='',\n",
              "       input_token_limit=32768,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemma-3-27b-it',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemma 3 27B',\n",
              "       description='',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemma-3n-e4b-it',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemma 3n E4B',\n",
              "       description='',\n",
              "       input_token_limit=8192,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemma-3n-e2b-it',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemma 3n E2B',\n",
              "       description='',\n",
              "       input_token_limit=8192,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='Gemini Flash Latest',\n",
              "       display_name='Gemini Flash Latest',\n",
              "       description='Latest release of Gemini Flash',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-flash-lite-latest',\n",
              "       base_model_id='',\n",
              "       version='Gemini Flash-Lite Latest',\n",
              "       display_name='Gemini Flash-Lite Latest',\n",
              "       description='Latest release of Gemini Flash-Lite',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='Gemini Pro Latest',\n",
              "       display_name='Gemini Pro Latest',\n",
              "       description='Latest release of Gemini Pro',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-lite',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 2.5 Flash-Lite',\n",
              "       description='Stable version of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-image-preview',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Nano Banana',\n",
              "       description='Gemini 2.5 Flash Preview Image',\n",
              "       input_token_limit=32768,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-image',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Nano Banana',\n",
              "       description='Gemini 2.5 Flash Preview Image',\n",
              "       input_token_limit=32768,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-preview-09-2025',\n",
              "       base_model_id='',\n",
              "       version='Gemini 2.5 Flash Preview 09-2025',\n",
              "       display_name='Gemini 2.5 Flash Preview Sep 2025',\n",
              "       description='Gemini 2.5 Flash Preview Sep 2025',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-lite-preview-09-2025',\n",
              "       base_model_id='',\n",
              "       version='2.5-preview-09-25',\n",
              "       display_name='Gemini 2.5 Flash-Lite Preview Sep 2025',\n",
              "       description='Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-robotics-er-1.5-preview',\n",
              "       base_model_id='',\n",
              "       version='1.5-preview',\n",
              "       display_name='Gemini Robotics-ER 1.5 Preview',\n",
              "       description='Gemini Robotics-ER 1.5 Preview',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-computer-use-preview-10-2025',\n",
              "       base_model_id='',\n",
              "       version='Gemini 2.5 Computer Use Preview 10-2025',\n",
              "       display_name='Gemini 2.5 Computer Use Preview 10-2025',\n",
              "       description='Gemini 2.5 Computer Use Preview 10-2025',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-embedding-exp-03-07',\n",
              "       base_model_id='',\n",
              "       version='exp-03-07',\n",
              "       display_name='Gemini Embedding Experimental 03-07',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=8192,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-embedding-exp',\n",
              "       base_model_id='',\n",
              "       version='exp-03-07',\n",
              "       display_name='Gemini Embedding Experimental',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=8192,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=40),\n",
              " Model(name='models/imagen-3.0-generate-002',\n",
              "       base_model_id='',\n",
              "       version='002',\n",
              "       display_name='Imagen 3.0',\n",
              "       description='Vertex served Imagen 3.0 002 model',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predict'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/imagen-4.0-generate-preview-06-06',\n",
              "       base_model_id='',\n",
              "       version='01',\n",
              "       display_name='Imagen 4 (Preview)',\n",
              "       description='Vertex served Imagen 4.0 model',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predict'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
              "       base_model_id='',\n",
              "       version='01',\n",
              "       display_name='Imagen 4 Ultra (Preview)',\n",
              "       description='Vertex served Imagen 4.0 ultra model',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predict'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/imagen-4.0-generate-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Imagen 4',\n",
              "       description='Vertex served Imagen 4.0 model',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predict'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/imagen-4.0-ultra-generate-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Imagen 4 Ultra',\n",
              "       description='Vertex served Imagen 4.0 ultra model',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predict'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/imagen-4.0-fast-generate-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Imagen 4 Fast',\n",
              "       description='Vertex served Imagen 4.0 Fast model',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predict'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/veo-2.0-generate-001',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Veo 2',\n",
              "       description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
              "                    'enabled on the associated Google Cloud Platform account. Please visit '\n",
              "                    'https://console.cloud.google.com/billing to enable it.'),\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predictLongRunning'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/veo-3.0-generate-preview',\n",
              "       base_model_id='',\n",
              "       version='3.0',\n",
              "       display_name='Veo 3',\n",
              "       description='Veo 3 preview.',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predictLongRunning'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/veo-3.0-fast-generate-preview',\n",
              "       base_model_id='',\n",
              "       version='3.0',\n",
              "       display_name='Veo 3 fast',\n",
              "       description='Veo 3 fast preview.',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predictLongRunning'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/veo-3.0-generate-001',\n",
              "       base_model_id='',\n",
              "       version='3.0',\n",
              "       display_name='Veo 3',\n",
              "       description='Veo 3',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predictLongRunning'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/veo-3.0-fast-generate-001',\n",
              "       base_model_id='',\n",
              "       version='3.0',\n",
              "       display_name='Veo 3 fast',\n",
              "       description='Veo 3 fast',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predictLongRunning'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/veo-3.1-generate-preview',\n",
              "       base_model_id='',\n",
              "       version='3.1',\n",
              "       display_name='Veo 3.1',\n",
              "       description='Veo 3.1',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predictLongRunning'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/veo-3.1-fast-generate-preview',\n",
              "       base_model_id='',\n",
              "       version='3.1',\n",
              "       display_name='Veo 3.1 fast',\n",
              "       description='Veo 3.1 fast',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predictLongRunning'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-2.5-flash-preview-native-audio-dialog',\n",
              "       base_model_id='',\n",
              "       version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
              "       display_name='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
              "       description='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-exp-native-audio-thinking-dialog',\n",
              "       base_model_id='',\n",
              "       version='gemini-2.5-flash-exp-native-audio-thinking-dialog-2025-05-19',\n",
              "       display_name='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
              "       description='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-live-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 2.0 Flash 001',\n",
              "       description='Gemini 2.0 Flash 001',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-live-2.5-flash-preview',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini Live 2.5 Flash Preview',\n",
              "       description='Gemini Live 2.5 Flash Preview',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-live-preview',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 2.5 Flash Live Preview',\n",
              "       description='Gemini 2.5 Flash Live Preview',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-native-audio-latest',\n",
              "       base_model_id='',\n",
              "       version='Gemini 2.5 Flash Native Audio Latest',\n",
              "       display_name='Gemini 2.5 Flash Native Audio Latest',\n",
              "       description='Latest release of Gemini 2.5 Flash Native Audio',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-native-audio-preview-09-2025',\n",
              "       base_model_id='',\n",
              "       version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
              "       display_name='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
              "       description='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To get a model separately\n",
        "# gen_ai_model = genai.GenerativeModel('gemini-2.0-flash')"
      ],
      "metadata": {
        "id": "J5MLyI04Rff6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating LLM**"
      ],
      "metadata": {
        "id": "pwv9iU90SNBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# Used when you want to use Gemini inside LangChain instead of directly through the Google library.\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    google_api_key = key,\n",
        "    temperature = 0,\n",
        "    max_output_tokens = 512\n",
        ")"
      ],
      "metadata": {
        "id": "K7KtllGYSCJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Agentic AI : ReAct Pattern or Tool use Pattern**\n",
        "- Reasoning + action alternately"
      ],
      "metadata": {
        "id": "O0tkqfX4UJ81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage, AIMessage\n",
        "\"\"\"\n",
        "These are structured message formats used in LangChain to store:\n",
        "- Messages from the user → HumanMessage\n",
        "- Responses from the AI → AIMessage\n",
        "This helps in chat history management, which is essential for multi-turn conversations.\n",
        "\"\"\"\n",
        "class Agent:\n",
        "  def __init__(self):\n",
        "    self.messages = [] #message history - maintain\n",
        "    \"\"\"\n",
        "    Creates an empty list to store past conversation messages.\n",
        "    \"\"\"\n",
        "\n",
        "  #dunder method\n",
        "  def __call__(self,message):\n",
        "    \"\"\"\n",
        "    This is a dunder method.\n",
        "    It allows the object to be called like a function:\n",
        "    - `agent(\"Hello\")` is same as `agent.__call__(\"Hello\")`\n",
        "    \"\"\"\n",
        "    self.messages.append(HumanMessage(content=message))\n",
        "    reponse = self.execute()\n",
        "    self.messages.append(AIMessage(content=reponse))\n",
        "    return reponse\n",
        "    \"\"\"\n",
        "    What it does:\n",
        "    - Stores the user input (HumanMessage)\n",
        "    - Calls execute() to get AI response\n",
        "    - Adds AI reply (AIMessage) to history\n",
        "    - Returns the response\n",
        "\n",
        "    This allows the agent to maintain memory across turns.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "  def execute(self):\n",
        "    response = llm(self.messages)\n",
        "    return response.content\n",
        "    \"\"\"\n",
        "    - Calls the LLM (e.g., Gemini, GPT, etc.)\n",
        "    - Sends the full conversation history\n",
        "    - Returns only the text content\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "vjlZmJPfSyi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "action_re = re.compile(r'Action: \\s*(\\w+):\\s*(.*?)(?:\\n|$)',re.MULTILINE)"
      ],
      "metadata": {
        "id": "798xtq9Yfjmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This regex is designed to extract structured information from text like:\n",
        "\n",
        "- Action: Move: forward\n",
        "- Action: Attack: dragon\n",
        "- Action: Speak: Hello there"
      ],
      "metadata": {
        "id": "e4vrw_PZf-P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Write code for the type of agent we want**"
      ],
      "metadata": {
        "id": "1dTXF4n3joI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate(expression):\n",
        "    return eval(expression)\n",
        "\n",
        "def convert_currency(data):\n",
        "    \"\"\"\n",
        "    Expects input like: \"<amount> <CURRENCY_CODE> to <TARGET_CURRENCY>\"\n",
        "    or a single pair like: \"10 USD\" (we'll assume convert TO INR if no target given).\n",
        "    Examples:\n",
        "      \"10 USD to INR\"\n",
        "      \"5 EUR to USD\"\n",
        "      \"100 JPY\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        parts = data.strip().split()\n",
        "        # Accept either \"AMOUNT SRC to TGT\" or \"AMOUNT SRC\"\n",
        "        if len(parts) == 2:\n",
        "            amount = float(parts[0])\n",
        "            src = parts[1].upper()\n",
        "            tgt = \"INR\"\n",
        "        elif len(parts) == 4 and parts[2].lower() == \"to\":\n",
        "            amount = float(parts[0])\n",
        "            src = parts[1].upper()\n",
        "            tgt = parts[3].upper()\n",
        "        else:\n",
        "            return \"Invalid input format. Use: '<amount> <SRC>' or '<amount> <SRC> to <TGT>'\"\n",
        "\n",
        "        # Static example rates (relative to 1 unit of the currency -> INR)\n",
        "        # These are sample/static values. For production use, query a live API.\n",
        "        inr_rates = {\n",
        "            \"USD\": 83.12,\n",
        "            \"EUR\": 89.45,\n",
        "            \"GBP\": 104.30,\n",
        "            \"AUD\": 55.60,\n",
        "            \"CAD\": 60.25,\n",
        "            \"JPY\": 0.56,\n",
        "            \"CNY\": 11.48,\n",
        "            \"AED\": 22.65,\n",
        "            \"SGD\": 61.15,\n",
        "            \"CHF\": 93.20,\n",
        "            \"HKD\": 10.65,\n",
        "            \"NZD\": 50.40,\n",
        "            \"KRW\": 0.064,\n",
        "            \"SEK\": 8.20,\n",
        "            \"NOK\": 8.10,\n",
        "            \"DKK\": 11.95,\n",
        "            \"RUB\": 0.88,\n",
        "            \"BRL\": 16.20,\n",
        "            \"ZAR\": 4.35,\n",
        "            \"INR\": 1.0  # base\n",
        "        }\n",
        "\n",
        "        def to_inr(value, src_code):\n",
        "            if src_code not in inr_rates:\n",
        "                return None\n",
        "            return value * inr_rates[src_code]\n",
        "\n",
        "        def from_inr(value_inr, tgt_code):\n",
        "            if tgt_code not in inr_rates:\n",
        "                return None\n",
        "            return value_inr / inr_rates[tgt_code]\n",
        "\n",
        "        # Convert src -> INR\n",
        "        src_to_inr = to_inr(amount, src)\n",
        "        if src_to_inr is None:\n",
        "            return f\"Source currency '{src}' not supported.\"\n",
        "\n",
        "        # If target is INR, we are done\n",
        "        if tgt == \"INR\":\n",
        "            return f\"{amount} {src} = {src_to_inr:.2f} INR\"\n",
        "\n",
        "        # Convert INR -> target\n",
        "        converted = from_inr(src_to_inr, tgt)\n",
        "        if converted is None:\n",
        "            return f\"Target currency '{tgt}' not supported.\"\n",
        "\n",
        "        return f\"{amount} {src} = {converted:.6f} {tgt} (≈ {src_to_inr:.2f} INR)\"\n",
        "    except Exception as e:\n",
        "        return f\"Error parsing input: {str(e)}. Use '<amount> <SRC>' or '<amount> <SRC> to <TGT>'.\"\n",
        "\n",
        "known_actions = {\n",
        "    \"calculate\": calculate,\n",
        "    \"convert_currency\": convert_currency\n",
        "}"
      ],
      "metadata": {
        "id": "EhF8fERxjs9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_query(prompt, max_turns = 10):  # Increased max_turns\n",
        "    agent = Agent()\n",
        "    \"\"\"\n",
        "    Starts an agent\n",
        "    \"\"\"\n",
        "    next_prompt = prompt\n",
        "\n",
        "    for turn in range(max_turns):\n",
        "        print(f\"Turn {turn + 1}:\")\n",
        "        response = agent(next_prompt)\n",
        "        print(f\"Assistant: \\n{response}\\n\")\n",
        "\n",
        "        \"\"\"\n",
        "        - Loops up to max_turns\n",
        "        - Sends prompt to the agent\n",
        "        - Prints the assistant’s response\n",
        "        \"\"\"\n",
        "\n",
        "        # Check for action lines in the response\n",
        "        actions = action_re.findall(response)\n",
        "\n",
        "        if actions:\n",
        "            action, action_input = actions[0]\n",
        "            print(f\"acrtion is :{actions}\")\n",
        "            action_input = action_input.strip()\n",
        "\n",
        "            if action not in known_actions:\n",
        "                raise ValueError(f\"Unknown action: {action}\")\n",
        "\n",
        "            observation = known_actions[action](action_input)\n",
        "            print(f\"Observation: {observation}\\n\")\n",
        "            next_prompt = f\"Observation: {observation}\"\n",
        "\n",
        "            \"\"\"\n",
        "            - If the LLM outputs something like \"Action: X\",\n",
        "            - The agent calls a real function from known_actions\n",
        "            - Sends the result back as \"Observation\"\n",
        "            \"\"\"\n",
        "        else:\n",
        "            # No more actions found, check if we have a final answer\n",
        "            if \"Answer:\" in response:\n",
        "                print(\"Final answer provided!\")\n",
        "                break\n",
        "            else:\n",
        "                print(\"No action found and no final answer. Stopping.\")\n",
        "                break\n",
        "        \"\"\"\n",
        "        Stops when the AI writes \"Answer: ...\"\n",
        "        Otherwise ends after max turns\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "eacCXVSlfzvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructions_and_question = \"\"\"\n",
        "You are a helpful assistant following a ReAct reasoning pattern.\n",
        "\n",
        "For each step:\n",
        "1. Use \"Thought:\" to think step by step about the problem\n",
        "2. If you need to perform a conversion or calculation, use 'Action: <action_name>: <input>' on a new line\n",
        "3. After each action, I will provide an 'Observation:' with the result\n",
        "4. Continue thinking and acting until you can provide a final 'Answer:'\n",
        "\n",
        "Available actions:\n",
        "- calculate: perform a Python calculation (e.g., calculate: 5 + 3)\n",
        "- convert_currency: Convert currencies. Use inputs like \"10 USD\", \"10 USD to INR\", or \"5 EUR to USD\".\n",
        "\n",
        "Important: Only perform ONE action at a time, then wait for the observation.\n",
        "\n",
        "Question: I have 10 USD and 20 EUR. What is their combined value in INR?\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "ySQYVrgRhLI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_query(instructions_and_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CXtD63gk74z",
        "outputId": "87dca128-5f54-4805-fa1e-b25060de1c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Turn 1:\n",
            "Assistant: \n",
            "Thought: Okay, I need to convert both USD and EUR to INR and then add them together. First, I'll convert 10 USD to INR.\n",
            "Action: convert_currency: 10 USD to INR\n",
            "\n",
            "acrtion is :[('convert_currency', '10 USD to INR')]\n",
            "Observation: 10.0 USD = 831.20 INR\n",
            "\n",
            "Turn 2:\n",
            "Assistant: \n",
            "Thought: Now I need to convert 20 EUR to INR.\n",
            "Action: convert_currency: 20 EUR to INR\n",
            "\n",
            "acrtion is :[('convert_currency', '20 EUR to INR')]\n",
            "Observation: 20.0 EUR = 1789.00 INR\n",
            "\n",
            "Turn 3:\n",
            "Assistant: \n",
            "Thought: Now I need to add the two INR values together to get the total.\n",
            "Action: calculate: 831.20 + 1789.00\n",
            "\n",
            "acrtion is :[('calculate', '831.20 + 1789.00')]\n",
            "Observation: 2620.2\n",
            "\n",
            "Turn 4:\n",
            "Assistant: \n",
            "Thought: I have converted both currencies to INR and added them.\n",
            "Answer: 10 USD and 20 EUR is equivalent to 2620.2 INR.\n",
            "\n",
            "Final answer provided!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QJ6Jix4Ok_j0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}